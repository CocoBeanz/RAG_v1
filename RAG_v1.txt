import os
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
import faiss
import PyPDF2
from docx import Document
import pandas as pd
import json
import markdown2
from bs4 import BeautifulSoup
from striprtf.striprtf import rtf_to_text
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from huggingface_hub import login

def load_knowledge_base(texts=None, file_dir=None):
    """Load documents from texts or files."""
    documents = []
    if texts:
        documents = texts
    elif file_dir:
        for filename in os.listdir(file_dir):
            file_path = os.path.join(file_dir, filename)
            try:
                if filename.endswith(".txt"):
                    with open(file_path, "r", encoding="utf-8") as file:
                        documents.append(file.read())
                elif filename.endswith(".pdf"):
                    with open(file_path, "rb") as file:
                        reader = PyPDF2.PdfReader(file)
                        text = "".join(page.extract_text() or "" for page in reader.pages)
                        documents.append(text)
                elif filename.endswith(".docx"):
                    doc = Document(file_path)
                    text = " ".join(para.text for para in doc.paragraphs if para.text)
                    documents.append(text)
                elif filename.endswith(".csv"):
                    df = pd.read_csv(file_path)
                    documents.extend(" ".join(row.astype(str)) for _, row in df.iterrows())
                elif filename.endswith(".json"):
                    with open(file_path, "r", encoding="utf-8") as file:
                        data = json.load(file)
                        documents.extend(str(item) for item in (data if isinstance(data, list) else [data]))
                elif filename.endswith(".md"):
                    with open(file_path, "r", encoding="utf-8") as file:
                        html = markdown2.markdown(file.read())
                        documents.append(BeautifulSoup(html, "html.parser").get_text())
                elif filename.endswith(".html"):
                    with open(file_path, "r", encoding="utf-8") as file:
                        documents.append(BeautifulSoup(file, "html.parser").get_text())
                elif filename.endswith(".rtf"):
                    with open(file_path, "r", encoding="utf-8") as file:
                        documents.append(rtf_to_text(file.read()))
                elif filename.endswith((".xlsx", ".xls", ".xlsm", ".xlsb")):
                    xl = pd.read_excel(file_path, sheet_name=None)
                    for df in xl.values():
                        documents.extend(" ".join(str(val) for val in row if pd.notnull(val)) 
                                       for _, row in df.iterrows() if any(pd.notnull(val) for val in row))
            except Exception as e:
                print(f"Error processing {filename}: {e}")
                continue
    if not documents:
        raise ValueError("No documents provided or found.")
    return documents

def build_rag_pipeline(documents, embedding_model_name="all-MiniLM-L6-v2", reranker_model_name="cross-encoder/ms-marco-MiniLM-L-6-v2", reranker_lora_path=None):
    """Build RAG pipeline with optional LoRA-adapted reranker."""
    embedding_model = SentenceTransformer(embedding_model_name)
    reranker = CrossEncoder(reranker_model_name)
    if reranker_lora_path:
        print("Loading LoRA-adapted reranker...")
        reranker.model = PeftModel.from_pretrained(reranker.model, reranker_lora_path)
    
    print("Generating embeddings...")
    embeddings = embedding_model.encode(documents, show_progress_bar=True).astype("float32")
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    print(f"Stored {index.ntotal} documents in FAISS.")
    
    return index, embedding_model, reranker, documents

def retrieve_and_rerank(query, index, embedding_model, reranker, documents, top_k=10, rerank_k=3):
    """Retrieve and rerank documents."""
    query_embedding = embedding_model.encode([query], show_progress_bar=False).astype("float32")
    distances, indices = index.search(query_embedding, top_k)
    retrieved_docs = [documents[idx] for idx in indices[0]]
    
    print("Reranking documents...")
    scores = reranker.predict([(query, doc) for doc in retrieved_docs])
    ranked_indices = np.argsort(scores)[::-1][:rerank_k]
    final_docs = [retrieved_docs[idx] for idx in ranked_indices]
    final_scores = scores[ranked_indices]
    
    return final_docs, final_scores

def generate_response(query, documents, llm_model_name="mistralai/Mistral-7B-v0.1", lora_path=None, hf_token=None):
    """Generate response using LLM with optional LoRA adapter and Hugging Face authentication."""
    # Authenticate with Hugging Face
    if hf_token:
        login(hf_token)
    else:
        # Check if token is set as environment variable
        hf_token = os.getenv("HF_TOKEN")
        if hf_token:
            login(hf_token)
        else:
            raise ValueError("Hugging Face token not provided. Set HF_TOKEN environment variable or pass hf_token.")

    base_model = AutoModelForCausalLM.from_pretrained(llm_model_name)
    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
    if lora_path:
        print("Loading LoRA-adapted LLM...")
        model = PeftModel.from_pretrained(base_model, lora_path)
    else:
        model = base_model
    
    prompt = f"Question: {query}\nContext: {' '.join(documents)}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def main():
    # Sample knowledge base
    sample_texts = [
        "Python is a high-level programming language known for its simplicity.",
        "Retrieval-Augmented Generation combines retrieval and generation for better answers.",
        "FAISS is a library for efficient similarity search and clustering of dense vectors.",
        "Sentence-BERT generates sentence embeddings for semantic search.",
        "RAG systems improve LLM performance by incorporating external knowledge."
    ]
    
    # Step 1: Load knowledge base
    documents = load_knowledge_base(texts=sample_texts)  # Or: file_dir="path/to/documents"
    
    # Step 2: Build RAG pipeline
    index, embedding_model, reranker, documents = build_rag_pipeline(
        documents, 
        reranker_lora_path=None  # Replace with "path/to/reranker/lora" if trained
    )
    
    # Step 3: Retrieve and rerank
    query = "What is Retrieval-Augmented Generation?"
    retrieved_docs, scores = retrieve_and_rerank(query, index, embedding_model, reranker, documents)
    
    # Step 4: Generate response
    response = generate_response(
        query, 
        retrieved_docs, 
        lora_path=None,  # Replace with "path/to/llm/lora" if trained
        hf_token=os.getenv("HF_TOKEN")  # Or pass token directly: "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    )
    
    # Print results
    print("\nTop Reranked Documents:")
    for i, (doc, score) in enumerate(zip(retrieved_docs, scores), 1):
        print(f"{i}. Score: {score:.4f} | {doc[:100]}...")
    print(f"\nLLM Response: {response}")

if __name__ == "__main__":
    main()
